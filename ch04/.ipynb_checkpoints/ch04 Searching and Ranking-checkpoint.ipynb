{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## crawler class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import urllib.parse\n",
    "from bs4 import BeautifulSoup\n",
    "import sqlite3\n",
    "import re\n",
    "\n",
    "# Create a list of words to ignore\n",
    "ignorewords={'the':1,'of':1,'to':1,'and':1,'a':1,'in':1,'is':1,'it':1}\n",
    "\n",
    "class Crawler:\n",
    "    # Initialize the crawler with the name of database\n",
    "    def __init__(self, db_name):\n",
    "        self.con = sqlite3.connect(db_name, timeout=10)\n",
    "    \n",
    "    def __del__(self):\n",
    "        self.con.close()\n",
    "        \n",
    "    def db_commit(self):\n",
    "        self.con.commit()\n",
    "    \n",
    "    # Axillary function for getting an entry id and adding\n",
    "    # it if it's not present\n",
    "    def get_entry_id(self, table, field, value, create_new=True):\n",
    "        cur=self.con.execute(\"select rowid from %s where %s='%s'\" % (table,field,value))\n",
    "        res=cur.fetchone()\n",
    "        if res==None:\n",
    "            cur=self.con.execute(\"insert into %s (%s) values ('%s')\" % (table,field,value))\n",
    "            return cur.lastrowid\n",
    "        else:\n",
    "            return res[0] \n",
    "    \n",
    "    # Create index for every page\n",
    "    def add_to_index(self, url, soup):\n",
    "        if self.is_indexed(url):\n",
    "            return \n",
    "        print('Indexing %s' % url)\n",
    "        \n",
    "        # Get the individual words\n",
    "        text = self.get_text_only(soup)\n",
    "        words = self.separate_words(text)\n",
    "        \n",
    "        # Get the id of URL\n",
    "        url_id = self.get_entry_id('urllist', 'url', url)\n",
    "        \n",
    "        # Link each word to this url\n",
    "        for i in range(len(words)):\n",
    "            word = words[i]\n",
    "            if word in ignorewords:\n",
    "                continue\n",
    "            word_id = self.get_entry_id('wordlist','word',word)\n",
    "            self.con.execute(\"insert into wordlocation(urlid,wordid,location) \\\n",
    "                             values (%d,%d,%d)\" % (url_id,word_id,i))\n",
    "        \n",
    "    # Extract the text from an HTML page (no tags)\n",
    "    def get_text_only(self, soup):\n",
    "        v = soup.string\n",
    "        if v==None:\n",
    "            c = soup.contents\n",
    "            result_text=''\n",
    "            for t in c:\n",
    "                sub_text = self.get_text_only(t)\n",
    "                result_text += sub_text + '\\n'\n",
    "            return result_text\n",
    "        else:\n",
    "            return v.strip()\n",
    "    \n",
    "    # Separate words by any non_whitespace character\n",
    "    def separate_words(self, text):\n",
    "        splitter = re.compile('\\\\W*')\n",
    "        return [s.lower() for s in splitter.split(text) if s!='']\n",
    "    \n",
    "    # Return true if the url is already indexed \n",
    "    def is_indexed(self, url):\n",
    "        u = self.con.execute(\"select rowid from urllist where url='%s'\" % url).fetchone()\n",
    "        if u != None:\n",
    "            v = self.con.execute(\"select * from wordlocation where urlid=%d\" % u[0]).fetchone()\n",
    "            if v != None:\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    # Add a link between two pages\n",
    "    def add_link_ref(self, urlFrom, urlTo, linkText):\n",
    "        pass\n",
    "    \n",
    "    # Starting with a list of pages, do a breadth \n",
    "    # first search to the given depth, indexing pages \n",
    "    # as we go\n",
    "    def crawl(self, pages, depth=2):\n",
    "        for i in range(depth):\n",
    "            print('depth %d begins' % i)\n",
    "            new_pages = set()\n",
    "            for page in pages:\n",
    "                try:\n",
    "                    c = urllib.request.urlopen(page)\n",
    "                except:\n",
    "                    print('Could not open %s' % page)\n",
    "                    continue\n",
    "                soup = BeautifulSoup(c.read(),'lxml')\n",
    "                self.add_to_index(page, soup)\n",
    "                \n",
    "                links = soup('a')\n",
    "                for link in links:\n",
    "                    if ('href' in dict(link.attrs)):\n",
    "                        url = urllib.parse.urljoin(page, link['href'])\n",
    "                        if url.find(\"'\")!=-1:\n",
    "                            continue\n",
    "                        url=url.split('#')[0]  # remove location portion\n",
    "                        if url[0:4]=='http' and not self.is_indexed(url):\n",
    "                            new_pages.add(url)\n",
    "                        link_text = self.get_text_only(link)\n",
    "                        self.add_link_ref(page, url, link_text)\n",
    "                self.db_commit()\n",
    "            pages = new_pages\n",
    "    \n",
    "    # Create the database tables\n",
    "    def create_index_tables(self):\n",
    "        self.con.execute('create table urllist(url)')\n",
    "        self.con.execute('create table wordlist(word)')\n",
    "        self.con.execute('create table wordlocation(urlid,wordid,location)')\n",
    "        self.con.execute('create table link(fromid integer,toid integer)')\n",
    "        self.con.execute('create table linkwords(wordid,linkid)')\n",
    "        self.con.execute('create index wordidx on wordlist(word)')\n",
    "        self.con.execute('create index urlidx on urllist(url)')\n",
    "        self.con.execute('create index wordurlidx on wordlocation(wordid)')\n",
    "        self.con.execute('create index urltoidx on link(toid)')\n",
    "        self.con.execute('create index urlfromidx on link(fromid)')\n",
    "        self.db_commit()\n",
    "        \n",
    "    def calculate_pagerank(self, iterations=20):\n",
    "        # clear out the current PageRank tables\n",
    "        self.con.execute('drop table if exists pagerank')\n",
    "        self.con.execute('create table pagerank(urlid primary key, score)')\n",
    "        \n",
    "        # Initialize every url with a pagerank of 1\n",
    "        self.con.execute('insert into pagerank select rowid, 1.0 from urllist')\n",
    "        self.con.dbcommit()\n",
    "        \n",
    "        for i in range(iterations):\n",
    "            print('Iteration %d' % i)\n",
    "            for (url_id,) in self.con.execute('select rowid from urllist'):\n",
    "                pr = 0.15\n",
    "                \n",
    "                # Loop through all the pages that link to this one\n",
    "                for (linker,) in self.con.execute('select distinct fromid from link where toid=%d' % urlid):\n",
    "                    # Get the pagerank of the linker \n",
    "                    linking_pr = self.con.execute('select score from pagerank where urlid=%d' % linker).fetchone()[0]\n",
    "                    \n",
    "                    # Get the total number of links from the linker\n",
    "                    linking_count = self.con.execute('select count(*) from link where fromid=%d' % linker).fetchone()[0]\n",
    "                    pr += 0.85*(linking_pr/linking_count)\n",
    "                self.con.execute('update pagerank set score=%f where urlid=%d' % (pr, urlid))\n",
    "            self.con.dbcommit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "crawler = Crawler('searchindex.db')\n",
    "# Run create_index_tables when db hasn't been created\n",
    "# crawler.create_index_tables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "depth 0 begins\n",
      "depth 1 begins\n",
      "Indexing http://www.bilibili.com/video/ent-food-1.html\n",
      "Indexing http://www.bilibili.com/video/tech-future-digital-1.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:69: FutureWarning: split() requires a non-empty pattern match.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing http://www.bilibili.com/video/music-coordinate-1.html\n",
      "Indexing http://www.bilibili.com/video/tech-popular-science-1.html\n",
      "Indexing http://www.bilibili.com/video/ent-sports-1.html\n",
      "Indexing http://www.bilibili.com/video/ent-variety-1.html\n",
      "Indexing http://www.bilibili.com/video/dance.html\n",
      "Indexing http://www.bilibili.com/video/kichiku-course-1.html\n",
      "Indexing http://www.bilibili.com/video/ad-ad-1.html\n",
      "Indexing http://www.bilibili.com/video/game.html\n",
      "Indexing http://www.bilibili.com/video/fashion-body-1.html\n",
      "Indexing http://www.bilibili.com/video/music-perform-1.html\n",
      "Indexing http://www.bilibili.com/video/technology.html\n",
      "Indexing http://www.bilibili.com/video/ent-handmake-1.html\n",
      "Indexing http://www.bilibili.com/video/speech-course-1.html\n",
      "Indexing http://www.bilibili.com/video/av120040/\n",
      "Indexing http://www.bilibili.com/video/dance-1.html\n",
      "Indexing http://www.bilibili.com/video/douga-else-1.html\n",
      "Indexing http://www.bilibili.com/video/douga-mmd-1.html\n",
      "Indexing http://www.bilibili.com/video/dance-demo-1.html\n",
      "Indexing http://www.bilibili.com/video/fashion-info-1.html\n",
      "Indexing http://www.bilibili.com/video/music.html\n",
      "Indexing http://www.bilibili.com/video/douga.html\n",
      "Indexing http://www.bilibili.com/video/music-Cover-1.html\n",
      "Indexing http://www.bilibili.com/video/ent_funny_1.html\n",
      "Indexing http://www.bilibili.com/video/life.html\n",
      "Indexing http://www.bilibili.com/video/music-oped-1.html\n",
      "Indexing http://www.bilibili.com/video/music-vocaloid-1.html\n",
      "Indexing http://www.bilibili.com/video/tech-future-other-1.html\n",
      "Indexing http://www.bilibili.com/random\n",
      "Indexing http://www.bilibili.com/video/three-dimension-dance-1.html\n",
      "Indexing http://www.bilibili.com/video/douga-kichiku-1.html\n",
      "Indexing http://www.bilibili.com/video/kichiku-manual_vocaloid-1.html\n",
      "Indexing http://www.bilibili.com/video/tech-future-military-1.html\n",
      "Indexing http://www.bilibili.com/video/tech-fun-1.html\n",
      "Indexing http://www.bilibili.com/mango\n",
      "Indexing http://www.bilibili.com/video/music-collection-1.html\n",
      "Indexing http://www.bilibili.com/video/ent-painting-1.html\n",
      "Indexing http://www.bilibili.com/video/music-original-1.html\n",
      "Indexing http://www.bilibili.com/video/tech-wild-1.html\n",
      "Indexing http://www.bilibili.com/video/fashion.html\n"
     ]
    }
   ],
   "source": [
    "page_list = ['http://www.bilibili.com']\n",
    "crawler.crawl(page_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 1),\n",
       " (760, 3),\n",
       " (1119, 9),\n",
       " (1393, 12),\n",
       " (1773, 13),\n",
       " (8365, 18),\n",
       " (8785, 20),\n",
       " (9325, 23),\n",
       " (9754, 27),\n",
       " (9848, 28),\n",
       " (10387, 37),\n",
       " (10392, 38),\n",
       " (12809, 41),\n",
       " (13589, 43),\n",
       " (13875, 46),\n",
       " (14239, 48),\n",
       " (14278, 49),\n",
       " (16346, 53),\n",
       " (16668, 54),\n",
       " (18110, 74),\n",
       " (18566, 81),\n",
       " (24459, 82),\n",
       " (24817, 88),\n",
       " (25579, 92),\n",
       " (27010, 100),\n",
       " (28498, 107),\n",
       " (29067, 111),\n",
       " (29787, 121),\n",
       " (29881, 123)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[row for row in crawler.con.execute('select rowid,urlid from wordlocation where wordid=2')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Searcher:\n",
    "    def __init__(self, db_name):\n",
    "        self.con = sqlite3.connect(db_name)\n",
    "    \n",
    "    def __del__(self):\n",
    "        self.con.close()\n",
    "        \n",
    "    def get_match_rows(self, q):\n",
    "        # Strings to build query\n",
    "        field_list = 'w0.urlid'\n",
    "        table_list = ''\n",
    "        clause_list = ''\n",
    "        word_ids = []\n",
    "        \n",
    "        # Split words by spaces\n",
    "        words = q.split(' ')\n",
    "        table_number = 0\n",
    "        \n",
    "        for word in words:\n",
    "            # get id of the word\n",
    "            word_row = self.con.execute(\"select rowid from wordlist where word = '%s'\" % word).fetchone()\n",
    "            if word_row != None:\n",
    "                word_id = word_row[0]\n",
    "                word_ids.append(word_id)\n",
    "                if table_number>0:\n",
    "                    table_list += ','\n",
    "                    clause_list += ' and '\n",
    "                    clause_list+='w%d.urlid=w%d.urlid and ' % (table_number-1,table_number)\n",
    "                field_list+=',w%d.location' % table_number\n",
    "                table_list+='wordlocation w%d' % table_number      \n",
    "                clause_list+='w%d.wordid=%d' % (table_number,word_id)\n",
    "                table_number+=1\n",
    "                \n",
    "        # Create the query from the separate parts\n",
    "        full_query='select %s from %s where %s' % (field_list,table_list,clause_list)\n",
    "        print (full_query)\n",
    "        cur=self.con.execute(full_query)\n",
    "        rows=[row for row in cur]\n",
    "        \n",
    "        return rows, word_ids\n",
    "    \n",
    "    def get_scored_list(self, rows, word_ids):\n",
    "        total_scores = dict([(row[0], 0) for row in rows])\n",
    "        \n",
    "        # score function\n",
    "        weighs = [(0.0, self.frequency_score(rows)),\n",
    "                 (1.0, self.location_score(rows)),\n",
    "                 (0.0, self.distance_score(rows)),\n",
    "                 (0.0, self.inbound_link_score(rows))]\n",
    "        \n",
    "        for (weight, scores) in weighs:\n",
    "            for url in total_scores:\n",
    "                total_scores[url] += weight*scores[url]\n",
    "                \n",
    "        return total_scores\n",
    "    \n",
    "    def get_url_name(self, id):\n",
    "        return self.con.execute(\"select url from urllist where rowid = %d\" % id).fetchone()[0]\n",
    "    \n",
    "    def query(self, q):\n",
    "        rows, word_ids = self.get_match_rows(q)\n",
    "        scores = self.get_scored_list(rows, word_ids)\n",
    "        ranked_scores = sorted([(score, url) for (url, score) in scores.items()], reverse=1)\n",
    "        for (score, url_id) in ranked_scores[0:10]:\n",
    "            print('%f\\t%s' % (score, self.get_url_name(url_id)))\n",
    "            \n",
    "    def normalize_scores(self, scores, small_is_better=0):\n",
    "        v_small = 0.00001 # Avoid division by zero errors\n",
    "        if small_is_better:\n",
    "            min_score = min(scores.values())\n",
    "            return dict([(u, float(min_score)/max(v_small,1)) for (u, l) in scores.items()])\n",
    "        else:\n",
    "            max_score = max(scores.values())\n",
    "            if max_score == 0:\n",
    "                max_score = v_small\n",
    "            return dict([(u, float(c)/max_score) for (u, c) in scores.items()])\n",
    "        \n",
    "    def frequency_score(self, rows):\n",
    "        counts = dict([(row[0], 0) for row in rows])\n",
    "        for row in rows:\n",
    "            counts[row[0]] += 1\n",
    "        return self.normalize_scores(counts)\n",
    "    \n",
    "    def location_score(self, rows):\n",
    "        locations = dict([(row[0], 0) for row in rows])\n",
    "        for row in rows:\n",
    "            loc = sum(row[1:])\n",
    "            if loc<locations[row[0]]:\n",
    "                locations[row[0]] = loc\n",
    "        return self.normalize_scores(locations, small_is_better=1)\n",
    "    \n",
    "    def distance_score(self, rows):\n",
    "        # if there's only one word, everyone wins!\n",
    "        if len(rows[0])<2:\n",
    "            return dict([(row[0], 1.0) for row in rows])\n",
    "        \n",
    "        # Initial dict with large values\n",
    "        min_distance = dict([(row[0], 1000000) for row in rows])\n",
    "        \n",
    "        for row in rows:\n",
    "            dist = sum([abs(row[i]-row[i-1]) for i in range(2, len(row))])\n",
    "            if dist<min_distance[row[0]]:\n",
    "                min_distance[row[0]] = dist\n",
    "        return self.normalize_scores(min_distance, small_is_better=1)\n",
    "    \n",
    "    def inbound_link_score(self, rows):\n",
    "        unique_urls = dict([(row[0], 1) for row in rows])\n",
    "        inbound_count = dict([(u,self.con.execute('select count(*) from link where toid=%d' % u)\n",
    "                                   .fetchone()[0]) for u in unique_urls])\n",
    "        return self.normalize_scores(inbound_count) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select w0.urlid,w0.location,w1.location from wordlocation w0,wordlocation w1 where w0.wordid=5 and w0.urlid=w1.urlid and w1.wordid=24\n",
      "0.000000\thttp://www.bilibili.com/html/friends-links.html\n",
      "0.000000\thttps://account.bilibili.com/login\n",
      "0.000000\thttp://live.bilibili.com/single\n",
      "0.000000\thttps://www.bilibili.com/register\n",
      "0.000000\thttp://live.bilibili.com/subject\n",
      "0.000000\thttp://bangumi.bilibili.com/movie/\n",
      "0.000000\thttp://h.bilibili.com/\n",
      "0.000000\thttp://game.bilibili.com\n",
      "0.000000\thttp://www.bilibili.com/html/contact.html\n",
      "0.000000\thttp://live.bilibili.com/mobile\n"
     ]
    }
   ],
   "source": [
    "e = Searcher('searchindex.db')\n",
    "e.query('bilibili 直播')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## PageRank "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "database is locked",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-9853f897e9d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcrawler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_pagerank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-4b5cb9bf2f5e>\u001b[0m in \u001b[0;36mcalculate_pagerank\u001b[0;34m(self, iterations)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcalculate_pagerank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;31m# clear out the current PageRank tables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'drop table if exists pagerank'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'create table pagerank(urlid primary key, score)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOperationalError\u001b[0m: database is locked"
     ]
    }
   ],
   "source": [
    "crawler.calculate_pagerank()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "crawler.con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
